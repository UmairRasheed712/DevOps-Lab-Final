name: CI/CD Pipeline - DevOps Final Lab

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  PYTHON_VERSION: '3.9'
  TERRAFORM_VERSION: '1.5.0'
  AWS_REGION: 'us-east-1'
  # EKS_CLUSTER_NAME is dynamic, retrieved from Terraform outputs
  DOCKER_IMAGE_NAME: 'devops-final-api'
  FRONTEND_IMAGE_NAME: 'devops-final-frontend'
  ENABLE_CODE_SCANNING_UPLOAD: 'false'

permissions:
  actions: read
  contents: read
  security-events: write

jobs:
  # Stage 1: Build & Test
  build-and-test:
    name: Build & Test Application
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: docker
          POSTGRES_DB: pfaas_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis:
        image: redis:7-alpine
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run unit tests with coverage
        env:
          POSTGRES_HOST_TEST: localhost
          POSTGRES_PORT_TEST: 5432
          POSTGRES_USER_TEST: postgres
          POSTGRES_PASSWORD_TEST: docker
          POSTGRES_DB_TEST: pfaas_test
          REDIS_HOST: localhost
          REDIS_PORT: 6379
          SECRET_KEY: test-secret-key-for-ci
        run: |
          pytest tests/ -v --cov=api --cov-report=xml --cov-report=html --cov-report=term
        continue-on-error: false

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          files: ./coverage.xml
          flags: unittests
          name: codecov-umbrella

      - name: Archive test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results
          path: |
            htmlcov/
            coverage.xml
          retention-days: 30

  # Stage 2: Security & Linting
  security-and-linting:
    name: Security Scanning & Code Quality
    runs-on: ubuntu-latest
    needs: build-and-test

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install linting tools
        run: |
          pip install flake8 pylint black isort bandit safety

      - name: Run Black (Code Formatting Check)
        run: |
          black --check --diff api/ tests/
        continue-on-error: true

      - name: Run isort (Import Sorting Check)
        run: |
          isort --check-only --diff api/ tests/
        continue-on-error: true

      - name: Run Flake8 (Linting)
        run: |
          flake8 api/ tests/ --count --select=E9,F63,F7,F82 --show-source --statistics
          flake8 api/ tests/ --count --max-complexity=10 --max-line-length=127 --statistics
        continue-on-error: true

      - name: Run Pylint (Code Quality)
        run: |
          pylint api/ --exit-zero
        continue-on-error: true

      - name: Run Bandit (Security Scanning)
        run: |
          bandit -r api/ -f json -o bandit-report.json
          bandit -r api/ -f screen
        continue-on-error: true

      - name: Check dependencies for vulnerabilities
        run: |
          safety check --json --output safety-report.json || true
          safety check || true
        continue-on-error: true

      - name: Run Trivy vulnerability scanner (Filesystem)
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          exit-code: '0'

      - name: Upload Trivy results to GitHub Security
        if: env.ENABLE_CODE_SCANNING_UPLOAD == 'true'
        uses: github/codeql-action/upload-sarif@v4
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Upload security reports
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: security-reports
          path: |
            bandit-report.json
            safety-report.json
            trivy-results.sarif
          retention-days: 30

  # Stage 3: Docker Build & Push
  docker-build-push:
    name: Build & Push Docker Image
    runs-on: ubuntu-latest
    needs: [build-and-test, security-and-linting]
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Create ECR repository if not exists
        run: |
          echo "Checking if ECR repository exists..."
          if ! aws ecr describe-repositories --repository-names ${{ env.DOCKER_IMAGE_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "Creating ECR repository: ${{ env.DOCKER_IMAGE_NAME }}"
            aws ecr create-repository \
              --repository-name ${{ env.DOCKER_IMAGE_NAME }} \
              --region ${{ env.AWS_REGION }} \
              --image-scanning-configuration scanOnPush=true \
              --encryption-configuration encryptionType=AES256
          else
            echo "ECR repository already exists"
          fi
          echo "ECR Registry: ${{ steps.login-ecr.outputs.registry }}"

      - name: Extract metadata for Docker
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/${{ env.DOCKER_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Docker image
        id: docker-build
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64
          provenance: false
          build-args: |
            BUILD_DATE=${{ github.event.head_commit.timestamp }}
            VCS_REF=${{ github.sha }}

      - name: Verify image was pushed
        run: |
          echo "Verifying image in ECR..."
          aws ecr describe-images \
            --repository-name ${{ env.DOCKER_IMAGE_NAME }} \
            --region ${{ env.AWS_REGION }} \
            --image-ids imageTag=latest || echo "Warning: Could not verify image"

      - name: Run Trivy vulnerability scanner on image
        uses: aquasecurity/trivy-action@master
        with:
          image-ref: ${{ steps.login-ecr.outputs.registry }}/${{ env.DOCKER_IMAGE_NAME }}:latest
          format: 'sarif'
          output: 'trivy-image-results.sarif'
          severity: 'CRITICAL,HIGH'
          exit-code: '0'

      - name: Upload Trivy image scan results
        if: env.ENABLE_CODE_SCANNING_UPLOAD == 'true'
        uses: github/codeql-action/upload-sarif@v4
        with:
          sarif_file: 'trivy-image-results.sarif'

      # --- Frontend Build Steps ---
      - name: Create ECR repository for Frontend if not exists
        run: |
          echo "Checking if ECR repository exists..."
          if ! aws ecr describe-repositories --repository-names ${{ env.FRONTEND_IMAGE_NAME }} --region ${{ env.AWS_REGION }} 2>/dev/null; then
            echo "Creating ECR repository: ${{ env.FRONTEND_IMAGE_NAME }}"
            aws ecr create-repository \
              --repository-name ${{ env.FRONTEND_IMAGE_NAME }} \
              --region ${{ env.AWS_REGION }} \
              --image-scanning-configuration scanOnPush=true \
              --encryption-configuration encryptionType=AES256
          else
            echo "ECR repository already exists"
          fi

      - name: Extract metadata for Frontend Docker
        id: meta-frontend
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/${{ env.FRONTEND_IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push Frontend Docker image
        id: docker-build-frontend
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          file: ./frontend/Dockerfile
          push: true
          tags: ${{ steps.meta-frontend.outputs.tags }}
          labels: ${{ steps.meta-frontend.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max
          platforms: linux/amd64
          provenance: false

  # Stage 4: Terraform Infrastructure Provisioning
  terraform-apply:
    name: Provision Infrastructure with Terraform
    runs-on: ubuntu-latest
    needs: docker-build-push
    defaults:
      run:
        working-directory: ./infra
    outputs:
      eks_cluster_name: ${{ steps.tf-outputs.outputs.eks_cluster_name }}
      rds_endpoint: ${{ steps.tf-outputs.outputs.rds_endpoint }}
      vpc_id: ${{ steps.tf-outputs.outputs.vpc_id }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Terraform Format Check
        run: terraform fmt -check -recursive
        continue-on-error: true

      - name: Terraform Init
        run: terraform init

      - name: Terraform Validate
        run: terraform validate

      - name: Terraform Plan
        run: |
          terraform plan \
            -var="db_password=${{ secrets.DB_PASSWORD }}" \
            -out=tfplan

      - name: Terraform Apply
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        run: |
          terraform apply -auto-approve tfplan

      - name: Save Terraform Outputs
        if: github.ref == 'refs/heads/main' && github.event_name == 'push'
        id: tf-outputs
        run: |
          echo "eks_cluster_name=$(terraform output -raw eks_cluster_name)" >> $GITHUB_OUTPUT
          echo "rds_endpoint=$(terraform output -raw rds_endpoint)" >> $GITHUB_OUTPUT
          echo "vpc_id=$(terraform output -raw vpc_id)" >> $GITHUB_OUTPUT

  # Stage 5: Deploy to Kubernetes
  deploy-to-kubernetes:
    name: Deploy Application to EKS
    runs-on: ubuntu-latest
    needs: terraform-apply
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.29.2'

      - name: Update kubeconfig
        run: |
          echo "=== AWS Identity Info ==="
          aws sts get-caller-identity
          
          echo "=== Discovering Cluster ==="
          CLUSTER_NAME="${{ needs.terraform-apply.outputs.eks_cluster_name }}"
          if [ -z "$CLUSTER_NAME" ] || [ "$CLUSTER_NAME" == "devops-final-eks" ]; then
            echo "Cluster name from output is empty or default. Attempting to discover..."
            CLUSTER_NAME=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?starts_with(@, 'devops-final-') && ends_with(@, '-eks')] | [0]" --output text)
          fi
          echo "Target Cluster: $CLUSTER_NAME"
          
          echo "=== Updating Kubeconfig ==="
          aws eks update-kubeconfig --name $CLUSTER_NAME --region ${{ env.AWS_REGION }}
          
          echo "=== Verifying Access ==="
          kubectl get svc --all-namespaces

      - name: Create Kubernetes namespace
        run: |
          kubectl create namespace devops-final --dry-run=client -o yaml | kubectl apply --validate=false -f -

      - name: Create Kubernetes secrets
        run: |
          kubectl create secret generic app-secrets \
            --from-literal=POSTGRES_PASSWORD=${{ secrets.DB_PASSWORD }} \
            --from-literal=SECRET_KEY=${{ secrets.JWT_SECRET }} \
            --namespace=devops-final \
            --dry-run=client -o yaml | kubectl apply --validate=false -f -

      - name: Update image tag in Kubernetes manifests
        run: |
          export IMAGE_TAG=${{ github.sha }}
          export ECR_REGISTRY=${{ steps.login-ecr.outputs.registry }}
          export POSTGRES_HOST=${{ needs.terraform-apply.outputs.rds_endpoint }}
          envsubst < k8s/configmap.yaml > k8s/configmap-updated.yaml
          envsubst < k8s/api-deployment.yaml > k8s/api-deployment-updated.yaml
          # Frontend manifest update is deferred until we have the API URL

      - name: Deploy Backend & Monitoring
        run: |
          kubectl apply -f k8s/configmap-updated.yaml -n devops-final --validate=false
          kubectl apply -f k8s/postgres.yaml -n devops-final --validate=false
          kubectl apply -f k8s/redis.yaml -n devops-final --validate=false
          kubectl apply -f k8s/api-deployment-updated.yaml -n devops-final --validate=false
          kubectl apply -f k8s/api-service.yaml -n devops-final --validate=false
          
          # Monitoring Stack
          kubectl apply -f k8s/monitoring/prometheus-config.yaml -n devops-final --validate=false
          kubectl apply -f k8s/monitoring/node-exporter.yaml -n devops-final --validate=false
          kubectl apply -f k8s/monitoring/prometheus-deployment.yaml -n devops-final --validate=false
          kubectl apply -f k8s/monitoring/prometheus-service.yaml -n devops-final --validate=false
          kubectl apply -f k8s/monitoring/grafana-deployment.yaml -n devops-final --validate=false
          kubectl apply -f k8s/monitoring/grafana-service.yaml -n devops-final --validate=false

      - name: Wait for API LoadBalancer
        continue-on-error: true
        run: |
          echo "Deployment applied - skipping rollout wait"
          sleep 3
          API_URL=""
          for i in {1..30}; do
            LB_HOSTNAME=$(kubectl get svc api-service -n devops-final -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
            if [ -n "$LB_HOSTNAME" ]; then
              API_URL="http://$LB_HOSTNAME:5000/api"
              break
            fi
            echo "Waiting for LoadBalancer..."
            sleep 10
          done
          
          if [ -z "$API_URL" ]; then
            echo "Could not get LoadBalancer URL. Using ClusterIP (internal only)."
            # Fallback for testing if LB fails
            API_URL="http://api-service:5000/api"
          fi
          
          echo "API_URL detected: $API_URL"
          echo "API_URL=$API_URL" >> $GITHUB_ENV

      - name: Deploy Frontend
        run: |
          export IMAGE_TAG=${{ github.sha }}
          export ECR_REGISTRY=${{ steps.login-ecr.outputs.registry }}
          # We need to manually replace the placeholder because envsubst doesn't see it as a variable
          sed "s|http://REPLACE_WITH_API_LB_URL/api|${{ env.API_URL }}|g" k8s/frontend-deployment.yaml > k8s/frontend-deployment-temp.yaml
          envsubst < k8s/frontend-deployment-temp.yaml > k8s/frontend-deployment-updated.yaml
          
          kubectl apply -f k8s/frontend-deployment-updated.yaml -n devops-final --validate=false
          kubectl apply -f k8s/frontend-service.yaml -n devops-final --validate=false

      - name: Get deployment status
        run: |
          kubectl get all -n devops-final
          kubectl get ingress -n devops-final || true

  # Stage 6: Post-Deploy Smoke Tests
  smoke-tests:
    name: Post-Deployment Smoke Tests
    runs-on: ubuntu-latest
    needs: [terraform-apply, deploy-to-kubernetes]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Manual Kubeconfig Setup
        run: |
          CLUSTER_NAME="${{ needs.terraform-apply.outputs.eks_cluster_name }}"
          if [ -z "$CLUSTER_NAME" ] || [ "$CLUSTER_NAME" == "devops-final-eks" ]; then
            echo "Cluster name from output is empty or default. Attempting to discover..."
            CLUSTER_NAME=$(aws eks list-clusters --region ${{ env.AWS_REGION }} --query "clusters[?starts_with(@, 'devops-final-') && ends_with(@, '-eks')] | [0]" --output text)
          fi
          echo "Using EKS Cluster: $CLUSTER_NAME"
          
          # Retrieve Cluster Info
          ENDPOINT=$(aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} --query "cluster.endpoint" --output text)
          CA_DATA=$(aws eks describe-cluster --name $CLUSTER_NAME --region ${{ env.AWS_REGION }} --query "cluster.certificateAuthority.data" --output text)
          
          # Generate Token
          TOKEN=$(aws eks get-token --cluster-name $CLUSTER_NAME --region ${{ env.AWS_REGION }} --output json | jq -r '.status.token')
          
          # Configure Kubectl
          echo $CA_DATA | base64 -d > ca.crt
          kubectl config set-cluster $CLUSTER_NAME --server=$ENDPOINT --certificate-authority=ca.crt --embed-certs=true
          kubectl config set-credentials deploy-user --token=$TOKEN
          kubectl config set-context $CLUSTER_NAME --cluster=$CLUSTER_NAME --user=deploy-user
          kubectl config use-context $CLUSTER_NAME

      - name: Install kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'v1.29.2'

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install test dependencies
        run: |
          pip install requests pytest

      - name: Wait for service to be ready
        run: |
          kubectl wait --for=condition=ready pod \
            -l app=api \
            -n devops-final \
            --timeout=300s

      - name: Get service endpoint
        id: endpoint
        run: |
          SERVICE_URL=$(kubectl get svc api-service -n devops-final -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
          if [ -z "$SERVICE_URL" ]; then
            SERVICE_URL=$(kubectl get svc api-service -n devops-final -o jsonpath='{.spec.clusterIP}')
            echo "endpoint=http://${SERVICE_URL}:5000" >> $GITHUB_OUTPUT
          else
            echo "endpoint=http://${SERVICE_URL}" >> $GITHUB_OUTPUT
          fi

      - name: Run smoke tests
        run: |
          chmod +x ./scripts/smoke-tests.sh
          ./scripts/smoke-tests.sh ${{ steps.endpoint.outputs.endpoint }}

      - name: Health check
        run: |
          ENDPOINT=${{ steps.endpoint.outputs.endpoint }}
          echo "Testing endpoint: $ENDPOINT"

          # Health check
          curl -f "${ENDPOINT}/health" || exit 1

          # GraphQL endpoint check
          curl -f "${ENDPOINT}/graphql" -H "Content-Type: application/json" \
            -d '{"query":"{ __schema { queryType { name } } }"}' || exit 1

      - name: Performance test
        run: |
          ENDPOINT=${{ steps.endpoint.outputs.endpoint }}

          # Simple load test
          for i in {1..10}; do
            curl -s "${ENDPOINT}/health" > /dev/null
            echo "Request $i completed"
          done

      - name: Get pod logs
        if: always()
        run: |
          kubectl logs -l app=api -n devops-final --tail=100 || true

      - name: Describe failing pods
        if: failure()
        run: |
          kubectl describe pods -n devops-final || true

  # Notification Job
  notify:
    name: Send Notifications
    runs-on: ubuntu-latest
    needs: [build-and-test, security-and-linting, docker-build-push, terraform-apply, deploy-to-kubernetes, smoke-tests]
    if: always()

    steps:
      - name: Send success notification
        if: ${{ needs.smoke-tests.result == 'success' }}
        run: |
          echo "üéâ Pipeline completed successfully!"
          echo "All stages passed: Build ‚úÖ Security ‚úÖ Docker ‚úÖ Terraform ‚úÖ Deploy ‚úÖ Tests ‚úÖ"

      - name: Send failure notification
        if: ${{ contains(needs.*.result, 'failure') }}
        run: |
          echo "‚ùå Pipeline failed!"
          echo "Please check the failed stages and fix the issues."
          exit 1
